{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pixAPI",
      "provenance": [],
      "collapsed_sections": [
        "xIGpknovzG0s",
        "SfDDAS3mzEea",
        "w5HGSfpHzOEP",
        "rdiRHjw2zWH6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Generalized-pix2pix-GAN-API/blob/master/pix2pixAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXlc4A-xMObg",
        "colab_type": "code",
        "outputId": "1f42aa0f-ea92-43f2-d0bf-51b55ee1bd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQh2Om35zKV2",
        "colab_type": "text"
      },
      "source": [
        "##init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J88tQp2L-ktx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import ImageFont\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from bokeh.io import curdoc, show, output_notebook\n",
        "from bokeh.layouts import column\n",
        "from bokeh.models import ColumnDataSource\n",
        "from bokeh.plotting import figure\n",
        "from functools import partial\n",
        "from threading import Thread\n",
        "from tornado import gen\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from utils import show_result,show_train_hist,generate_animation,data_load,imgs_resize, random_crop, random_fliplr\n",
        "from data_loader import localImageDataset\n",
        "from model import generator, discriminator, normal_init\n",
        "\n",
        "#root folder\n",
        "root_dir=\"/NAVA_ocr/\"\n",
        "\n",
        "#data directories\n",
        "#output images\n",
        "output_dir=root_dir+\"output/epoch/\"\n",
        "#input images\n",
        "input_dir=root_dir+\"data/augmented/\"\n",
        "\n",
        "#models\n",
        "model_dir=root_dir+\"model/\"\n",
        "#other resources\n",
        "res_dir=root_dir+\"res/\"\n",
        "#report and logging\n",
        "report_dir=root_dir+\"report/\"\n",
        "\n",
        "\n",
        "#parameters\n",
        "batch_size=1\n",
        "train_split=0.99\n",
        "train_epoch=10000\n",
        "\n",
        "#input\n",
        "data_dir=\"data\"\n",
        "inp_width=256\n",
        "inp_height=256\n",
        "inp_channels=3\n",
        "\n",
        "#generator\n",
        "ngf=2\n",
        "ndf=2\n",
        "\n",
        "#discriminator\n",
        "ndf=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdiRHjw2zWH6",
        "colab_type": "text"
      },
      "source": [
        "##load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtC--bqM-neS",
        "colab_type": "code",
        "outputId": "f71dbc49-a506-4a60-9313-b1affe77c115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Data loaders init\n",
        "root_dir=\"/content/drive/My Drive/Projects/Lumentum/NAVA_ocr/Lumentum/NAVA_ocr/\"\n",
        "dataset=localImageDataset(root_dir, inp_width, inp_height, inp_channels)\n",
        "print(len(dataset))\n",
        "train_size=int(train_split*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "train_dataset, val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                              batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)\n",
        "num_batches=len(train_dataloader)\n",
        "val_dataloader=torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True,\n",
        "                                          num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gmK38wG4woz",
        "colab_type": "text"
      },
      "source": [
        "##model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ74IrUB4vhW",
        "colab_type": "code",
        "outputId": "007267ef-7c21-4048-917a-f784c94b911a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#from model import generator, discriminator\n",
        "#import utils\n",
        "\n",
        "#parameters\n",
        "lrG=0.02\n",
        "lrD=0.02\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "L1_lambda=1.5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = generator(2)\n",
        "D = discriminator(2)\n",
        "\n",
        "start_time=time.time()\n",
        "epoch_start=0\n",
        "epoch_end=epoch_start+train_epoch\n",
        "\n",
        "#loss\n",
        "BCE_loss=nn.BCELoss().cuda()\n",
        "L1_loss=nn.L1Loss().cuda()\n",
        "G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "  \n",
        "  G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "  D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "  G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "  D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "  G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "  D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "  \n",
        "  train_hist=G_checkpoint['train_hist']\n",
        "  epoch_start=G_checkpoint['epoch']\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "else:\n",
        "  G.weight_init(mean=0.0, std=0.02)\n",
        "  D.weight_init(mean=0.0, std=0.02)\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  \n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  train_hist={}\n",
        "  train_hist['D_losses']=[]\n",
        "  train_hist['G_losses']=[]\n",
        "  train_hist['per_epoch_ptimes']=[]\n",
        "  train_hist['total_ptime']=[]\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "#import networks\n",
        "#from networks import generator, discriminator\n",
        "\n",
        "#parameters\n",
        "lrG=0.02\n",
        "lrD=0.02\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "L1_lambda=1.5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = generator(2)\n",
        "D = discriminator(2)\n",
        "G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "#loss\n",
        "BCE_loss=nn.BCELoss().to(device)\n",
        "L1_loss=nn.L1Loss().to(device)\n",
        "\n",
        "if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "  \n",
        "  G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "  D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "  G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "  D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  #D.eval()\n",
        "\n",
        "  G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "  D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "  \n",
        "  train_hist=G_checkpoint['train_hist']\n",
        "  epoch_start=G_checkpoint['epoch']\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "else:\n",
        "  print(\"Previous model not found. Restarting train process...\")\n",
        "  G.weight_init(mean=0.0, std=0.02)\n",
        "  D.weight_init(mean=0.0, std=0.02)\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  \n",
        "  \n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  train_hist={}\n",
        "  train_hist['D_losses']=[]\n",
        "  train_hist['G_losses']=[]\n",
        "  train_hist['per_epoch_ptimes']=[]\n",
        "  train_hist['total_ptime']=[]\n",
        "  epoch_start=0\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "\n",
        "for epoch in range(epoch_start,epoch_end):\n",
        "  D_losses=[]\n",
        "  G_losses=[]\n",
        "  epoch_start_time=time.time()\n",
        "  num_iter=0\n",
        "  for text_image, inp_image in train_dataloader:\n",
        "    inp_image,text_image=Variable(inp_image.to(device)),Variable(text_image.to(device))\n",
        "    D.zero_grad()\n",
        "\n",
        "    \n",
        "    D_result=D(inp_image,text_image).squeeze()\n",
        "    D_real_loss=BCE_loss(D_result,Variable(torch.ones(D_result.size()).to(device)))\n",
        "    \n",
        "    G_result=G(inp_image)\n",
        "    D_result=D(inp_image,G_result).squeeze()\n",
        "    D_fake_loss=BCE_loss(D_result,Variable(torch.zeros(D_result.size()).to(device)))\n",
        "    \n",
        "    D_train_loss=(D_real_loss +D_fake_loss)*0.5\n",
        "    D_train_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    train_hist['D_losses'].append(float(D_train_loss))\n",
        "    \n",
        "    D_losses.append(float(D_train_loss))\n",
        "    D_losses.append(float(0))\n",
        "    \n",
        "    #training generator\n",
        "    G.zero_grad()\n",
        "\n",
        "    G_result=G(inp_image)\n",
        "    D_result=D(text_image,G_result).squeeze()\n",
        "\n",
        "    G_train_loss=BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device))) + L1_lambda*L1_loss(G_result,text_image)\n",
        "    G_train_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    train_hist['G_losses'].append(float(G_train_loss))\n",
        "    G_losses.append(float(G_train_loss))\n",
        "    num_iter+=1\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            'train_hist': train_hist\n",
        "            }, model_dir+'generator_param.pkl')\n",
        "\n",
        "  torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            },model_dir+'discriminator_param.pkl')\n",
        "\n",
        "  epoch_end_time=time.time()\n",
        "  per_epoch_ptime=epoch_end_time-epoch_start_time\n",
        "  print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses))))\n",
        "  fixed_p =  output_dir  + str(epoch + 1) + '.png'\n",
        "  show_result(G, Variable(inp_image.to(device), volatile=True), text_image.cpu(), (epoch+1), save=True, path=fixed_p)\n",
        "  train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "  \n",
        "end_time=time.time()\n",
        "total_ptime=end_time-start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "  \n",
        "\n",
        "with open(report_dir+'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=report_dir + 'train_hist.png')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Previous model not found. Restarting train process...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY9cg48SaQ4s",
        "colab_type": "text"
      },
      "source": [
        "##Not required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V2ORgAI3n_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import networks\n",
        "#from networks import generator, discriminator\n",
        "\n",
        "#parameters\n",
        "lrG=0.02\n",
        "lrD=0.02\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "L1_lambda=1.5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = generator(2)\n",
        "D = discriminator(2)\n",
        "G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "#loss\n",
        "BCE_loss=nn.BCELoss().to(device)\n",
        "L1_loss=nn.L1Loss().to(device)\n",
        "\n",
        "if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "  \n",
        "  G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "  D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "  G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "  D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  #D.eval()\n",
        "\n",
        "  G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "  D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "  \n",
        "  train_hist=G_checkpoint['train_hist']\n",
        "  epoch_start=G_checkpoint['epoch']\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "else:\n",
        "  print(\"Previous model not found. Restarting train process...\")\n",
        "  G.weight_init(mean=0.0, std=0.02)\n",
        "  D.weight_init(mean=0.0, std=0.02)\n",
        "  G.to(device)\n",
        "  D.to(device)\n",
        "  G.train()\n",
        "  D.train()\n",
        "  \n",
        "  \n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  train_hist={}\n",
        "  train_hist['D_losses']=[]\n",
        "  train_hist['G_losses']=[]\n",
        "  train_hist['per_epoch_ptimes']=[]\n",
        "  train_hist['total_ptime']=[]\n",
        "  epoch_start=0\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "start_time=time.time()\n",
        "for epoch in range(epoch_start,epoch_end):\n",
        "  D_losses=[]\n",
        "  G_losses=[]\n",
        "  epoch_start_time=time.time()\n",
        "  num_iter=0\n",
        "  for text_image, inp_image in train_dataloader:\n",
        "    inp_image,text_image=Variable(inp_image.to(device)),Variable(text_image.to(device))\n",
        "    D.zero_grad()\n",
        "\n",
        "    \n",
        "    D_result=D(inp_image,text_image).squeeze()\n",
        "    D_real_loss=BCE_loss(D_result,Variable(torch.ones(D_result.size()).to(device)))\n",
        "    \n",
        "    G_result=G(inp_image)\n",
        "    D_result=D(inp_image,G_result).squeeze()\n",
        "    D_fake_loss=BCE_loss(D_result,Variable(torch.zeros(D_result.size()).to(device)))\n",
        "    \n",
        "    D_train_loss=(D_real_loss +D_fake_loss)*0.5\n",
        "    D_train_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    train_hist['D_losses'].append(float(D_train_loss))\n",
        "    \n",
        "    D_losses.append(float(D_train_loss))\n",
        "    D_losses.append(float(0))\n",
        "    \n",
        "    #training generator\n",
        "    G.zero_grad()\n",
        "\n",
        "    G_result=G(inp_image)\n",
        "    D_result=D(text_image,G_result).squeeze()\n",
        "\n",
        "    G_train_loss=BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device))) + L1_lambda*L1_loss(G_result,text_image)\n",
        "    G_train_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    train_hist['G_losses'].append(float(G_train_loss))\n",
        "    G_losses.append(float(G_train_loss))\n",
        "    num_iter+=1\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            'train_hist': train_hist\n",
        "            }, model_dir+'generator_param.pkl')\n",
        "\n",
        "  torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            },model_dir+'discriminator_param.pkl')\n",
        "\n",
        "  epoch_end_time=time.time()\n",
        "  per_epoch_ptime=epoch_end_time-epoch_start_time\n",
        "  print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses))))\n",
        "  fixed_p =  output_dir  + str(epoch + 1) + '.png'\n",
        "  show_result(G, Variable(inp_image.to(device), volatile=True), text_image.cpu(), (epoch+1), save=True, path=fixed_p)\n",
        "  train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "  \n",
        "end_time=time.time()\n",
        "total_ptime=end_time-start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "  \n",
        "\n",
        "with open(report_dir+'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=report_dir + 'train_hist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJphJazZ8QT",
        "colab_type": "text"
      },
      "source": [
        "##Complete function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a7FfKvynlf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import ImageFont\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from bokeh.io import curdoc, show, output_notebook\n",
        "from bokeh.layouts import column\n",
        "from bokeh.models import ColumnDataSource\n",
        "from bokeh.plotting import figure\n",
        "from functools import partial\n",
        "from threading import Thread\n",
        "from tornado import gen\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from utils import show_result,show_train_hist,generate_animation,data_load,imgs_resize, random_crop, random_fliplr\n",
        "from data_loader import localImageDataset\n",
        "from model import generator, discriminator, normal_init\n",
        "\n",
        "#root folder\n",
        "root_dir=\"/NAVA_ocr/\"\n",
        "\n",
        "#data directories\n",
        "#output images\n",
        "output_dir=root_dir+\"output/epoch/\"\n",
        "#input images\n",
        "input_dir=root_dir+\"data/augmented/\"\n",
        "\n",
        "#models\n",
        "model_dir=root_dir+\"model/\"\n",
        "#other resources\n",
        "res_dir=root_dir+\"res/\"\n",
        "#report and logging\n",
        "report_dir=root_dir+\"report/\"\n",
        "\n",
        "\n",
        "#parameters\n",
        "batch_size=1\n",
        "train_split=0.99\n",
        "train_epoch=10000\n",
        "\n",
        "#input\n",
        "data_dir=\"data\"\n",
        "inp_width=256\n",
        "inp_height=256\n",
        "inp_channels=3\n",
        "\n",
        "#generator\n",
        "ngf=2\n",
        "ndf=2\n",
        "\n",
        "#discriminator\n",
        "ndf=2\n",
        "\n",
        "#Data loaders init\n",
        "root_dir=\"/content/drive/My Drive/Projects/Lumentum/NAVA_ocr/Lumentum/NAVA_ocr/\"\n",
        "root_dir=\"/content/drive/My Drive/Projects/pix2pix API/\"\n",
        "\n",
        "def complete(root_dir):\n",
        "  dataset=localImageDataset(root_dir, inp_width, inp_height, inp_channels)\n",
        "  print(len(dataset))\n",
        "  train_size=int(train_split*len(dataset))\n",
        "  val_size=len(dataset)-train_size\n",
        "  train_dataset, val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "  train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                                batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                            num_workers=4)\n",
        "  num_batches=len(train_dataloader)\n",
        "  val_dataloader=torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                            num_workers=4)\n",
        "\n",
        "\n",
        "  #from model import generator, discriminator\n",
        "  #import utils\n",
        "\n",
        "  #parameters\n",
        "  lrG=0.02\n",
        "  lrD=0.02\n",
        "  beta1=0.5\n",
        "  beta2=0.999\n",
        "  L1_lambda=1.5\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  G = generator(2)\n",
        "  D = discriminator(2)\n",
        "\n",
        "  start_time=time.time()\n",
        "  epoch_start=0\n",
        "  epoch_end=epoch_start+train_epoch\n",
        "\n",
        "  #loss\n",
        "  BCE_loss=nn.BCELoss().cuda()\n",
        "  L1_loss=nn.L1Loss().cuda()\n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "  if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "\n",
        "    G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "    D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "    G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "    D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "    G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "    D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    train_hist=G_checkpoint['train_hist']\n",
        "    epoch_start=G_checkpoint['epoch']\n",
        "    epoch_end=epoch_start+train_epoch\n",
        "  else:\n",
        "    G.weight_init(mean=0.0, std=0.02)\n",
        "    D.weight_init(mean=0.0, std=0.02)\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "    G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "    D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "    train_hist={}\n",
        "    train_hist['D_losses']=[]\n",
        "    train_hist['G_losses']=[]\n",
        "    train_hist['per_epoch_ptimes']=[]\n",
        "    train_hist['total_ptime']=[]\n",
        "    epoch_end=epoch_start+train_epoch\n",
        "\n",
        "  #import networks\n",
        "  #from networks import generator, discriminator\n",
        "\n",
        "  #parameters\n",
        "  lrG=0.02\n",
        "  lrD=0.02\n",
        "  beta1=0.5\n",
        "  beta2=0.999\n",
        "  L1_lambda=1.5\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  G = generator(2)\n",
        "  D = discriminator(2)\n",
        "  G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "  D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "  #loss\n",
        "  BCE_loss=nn.BCELoss().to(device)\n",
        "  L1_loss=nn.L1Loss().to(device)\n",
        "\n",
        "  if(os.path.isfile(model_dir+'generator_param.pkl') and os.path.isfile(model_dir+'discriminator_param.pkl')):\n",
        "\n",
        "    G_checkpoint=torch.load(model_dir+'generator_param.pkl',map_location=device)\n",
        "    D_checkpoint=torch.load(model_dir+'discriminator_param.pkl',map_location=device)\n",
        "    G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "    D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "    G.train()\n",
        "    D.train()\n",
        "    #D.eval()\n",
        "\n",
        "    G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])\n",
        "    D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    train_hist=G_checkpoint['train_hist']\n",
        "    epoch_start=G_checkpoint['epoch']\n",
        "    epoch_end=epoch_start+train_epoch\n",
        "  else:\n",
        "    print(\"Previous model not found. Restarting train process...\")\n",
        "    G.weight_init(mean=0.0, std=0.02)\n",
        "    D.weight_init(mean=0.0, std=0.02)\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "\n",
        "    G_optimizer=optim.Adam(G.parameters(),lr=lrG,betas=(beta1,beta2))\n",
        "    D_optimizer=optim.Adam(D.parameters(),lr=lrD,betas=(beta1,beta2))\n",
        "\n",
        "    train_hist={}\n",
        "    train_hist['D_losses']=[]\n",
        "    train_hist['G_losses']=[]\n",
        "    train_hist['per_epoch_ptimes']=[]\n",
        "    train_hist['total_ptime']=[]\n",
        "    epoch_start=0\n",
        "    epoch_end=epoch_start+train_epoch\n",
        "\n",
        "\n",
        "  for epoch in range(epoch_start,epoch_end):\n",
        "    D_losses=[]\n",
        "    G_losses=[]\n",
        "    epoch_start_time=time.time()\n",
        "    num_iter=0\n",
        "    for text_image, inp_image in train_dataloader:\n",
        "      inp_image,text_image=Variable(inp_image.to(device)),Variable(text_image.to(device))\n",
        "      D.zero_grad()\n",
        "\n",
        "\n",
        "      D_result=D(inp_image,text_image).squeeze()\n",
        "      D_real_loss=BCE_loss(D_result,Variable(torch.ones(D_result.size()).to(device)))\n",
        "\n",
        "      G_result=G(inp_image)\n",
        "      D_result=D(inp_image,G_result).squeeze()\n",
        "      D_fake_loss=BCE_loss(D_result,Variable(torch.zeros(D_result.size()).to(device)))\n",
        "\n",
        "      D_train_loss=(D_real_loss +D_fake_loss)*0.5\n",
        "      D_train_loss.backward()\n",
        "      D_optimizer.step()\n",
        "      train_hist['D_losses'].append(float(D_train_loss))\n",
        "\n",
        "      D_losses.append(float(D_train_loss))\n",
        "      D_losses.append(float(0))\n",
        "\n",
        "      #training generator\n",
        "      G.zero_grad()\n",
        "\n",
        "      G_result=G(inp_image)\n",
        "      D_result=D(text_image,G_result).squeeze()\n",
        "\n",
        "      G_train_loss=BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device))) + L1_lambda*L1_loss(G_result,text_image)\n",
        "      G_train_loss.backward()\n",
        "      G_optimizer.step()\n",
        "\n",
        "      train_hist['G_losses'].append(float(G_train_loss))\n",
        "      G_losses.append(float(G_train_loss))\n",
        "      num_iter+=1\n",
        "\n",
        "    torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': G.state_dict(),\n",
        "              'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "              'train_hist': train_hist\n",
        "              }, model_dir+'generator_param.pkl')\n",
        "\n",
        "    torch.save({\n",
        "              'model_state_dict': D.state_dict(),\n",
        "              'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "              },model_dir+'discriminator_param.pkl')\n",
        "\n",
        "    epoch_end_time=time.time()\n",
        "    per_epoch_ptime=epoch_end_time-epoch_start_time\n",
        "    print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                                torch.mean(torch.FloatTensor(G_losses))))\n",
        "    fixed_p =  output_dir  + str(epoch + 1) + '.png'\n",
        "    show_result(G, Variable(inp_image.to(device), volatile=True), text_image.cpu(), (epoch+1), save=True, path=fixed_p)\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "  end_time=time.time()\n",
        "  total_ptime=end_time-start_time\n",
        "  train_hist['total_ptime'].append(total_ptime)\n",
        "  print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "\n",
        "\n",
        "  with open(report_dir+'train_hist.pkl', 'wb') as f:\n",
        "      pickle.dump(train_hist, f)\n",
        "\n",
        "  show_train_hist(train_hist, save=True, path=report_dir + 'train_hist.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}